# -*- coding: utf-8 -*-
"""
QualiJournal Admin API + Lite Black UI (stable, cleaned)
- Community / Keyword / Daily flows (sync & async)
- Async Task Manager (/api/tasks/*) + SSE stream
- Gate config API (GET/PATCH /api/config/gate_required)
- Report & Export (/api/report, /api/export/{md|csv})
- Log viewing (/api/logs/*)
- UTF-8 safe on Windows; non-breaking fallback if optional modules are missing.
"""

from __future__ import annotations

import os
import sys
import json
import csv
import io
import subprocess
import datetime as _dt
import hashlib
import asyncio
import threading
import secrets
import time
import re
from pathlib import Path
from typing import List, Dict, Any, Optional

# FastAPI / Pydantic
from fastapi import FastAPI, Query, Response, HTTPException, Depends, Body, Request
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import HTMLResponse, PlainTextResponse, FileResponse, StreamingResponse, JSONResponse
from pydantic import BaseModel, Field
from starlette.staticfiles import StaticFiles

# SSOT: ì„œë¸Œí”„ë¡œì„¸ìŠ¤ë„ ì´ íŒŒì´ì¬ìœ¼ë¡œ ì‹¤í–‰
PYEXE = os.getenv("PYTHON_EXE") or sys.executable or "python"

# ---------------------------------------------------------------------------
# .env (optional) - load first; define MODE fallback regardless of availability
# ---------------------------------------------------------------------------
try:
    from dotenv import load_dotenv  # type: ignore
    load_dotenv()
except Exception:
    # dotenv is optional; ignore if not present
    pass

MODE = (os.getenv("QUALI_DB_MODE") or "local").lower().strip()

# admin.db (optional). If unavailable, db endpoints gracefully degrade.
try:
    from admin.db import make_engine  # type: ignore
    _engine = None
except Exception:  # pragma: no cover
    make_engine = None  # type: ignore
    _engine = None

# === FastAPI app (define FIRST) ==============================================
app = FastAPI(title="QualiJournal Admin API")
# ---------------------------------------------------------------------------
# Models
# ---------------------------------------------------------------------------
class EnrichReq(BaseModel):
    date: Optional[str] = None
    keyword: Optional[str] = None
    mode: Optional[str] = "keyword"  # "keyword" or "selection"
    items: Optional[List[Dict[str, Any]]] = None

class GatePatch(BaseModel):
    gate_required: int

class PublishOneReq(BaseModel):
    approve: bool = Field(default=True, description="ìŠ¹ì¸ ì—¬ë¶€")
    editor_note: Optional[str] = Field(default=None, description="í¸ì§‘ì¥ í•œë§ˆë””(ì„ íƒ)")

class TaskItem(BaseModel):
    id: str
    size: int

class TasksRecent(BaseModel):
    items: List[TaskItem]

class ReportReq(BaseModel):
    """ìš”ì²­ ë³¸ë¬¸: date(ì„ íƒ), keyword(ì„ íƒ)"""
    date: str | None = None
    keyword: str | None = None

class FlowReq(BaseModel):
    kind: str            # daily|community|keyword
    keyword: str | None = None

class PublishReq(BaseModel):
    keyword: str

class FlowKwReq(BaseModel):
    keyword: str
    use_external_rss: bool = False

# ---------------------------------------------------------------------------
# Optional JWT utils (safe fallbacks if module missing)
# ---------------------------------------------------------------------------
try:
    from auth_utils import verify_jwt_token  # type: ignore
except Exception:  # pragma: no cover
    async def verify_jwt_token(*args, **kwargs):  # type: ignore
        return {}

# Simple Bearer Token Authorization (Cloud Run OIDC + App Token ë™ì‹œ ì§€ì›)
security = HTTPBearer(auto_error=False)

async def authorize(
    request: Request,
    credentials: HTTPAuthorizationCredentials = Depends(security)
) -> bool:
    """
    í—ˆìš© ê·œì¹™
    - ADMIN_TOKEN ë˜ëŠ” API_TOKEN ë‘˜ ì¤‘ í•˜ë‚˜ë¼ë„ ì„¤ì •ë˜ì–´ ìˆì§€ ì•Šìœ¼ë©´ open mode(í†µê³¼)
    - ì„¤ì •ë˜ì–´ ìˆìœ¼ë©´ ë‹¤ìŒ ì¤‘ 'í•˜ë‚˜'ë¼ë„ ë§ìœ¼ë©´ í†µê³¼
      1) í—¤ë” X-Admin-Token: <ADMIN_TOKEN or API_TOKEN>
      2) Authorization: Bearer <ADMIN_TOKEN or API_TOKEN>   (ë ˆê±°ì‹œ í˜¸í™˜)
    - Cloud Run ë¹„ê³µê°œ ì„œë¹„ìŠ¤ì—ì„œ Authorizationì€ ë³´í†µ 'ID í† í°'ì´ë¯€ë¡œ,
      ì´ ê²½ìš° X-Admin-Token ìœ¼ë¡œ ì•± í† í°ì„ ë”°ë¡œ ì‹¤ì–´ì•¼ í†µê³¼ë¨.
    """
    expected = [
        (os.environ.get("ADMIN_TOKEN") or "").strip(),
        (os.environ.get("API_TOKEN") or "").strip(),
    ]
    expected = [t for t in expected if t]
    if not expected:
        return True  # open mode

    # ì•± ì „ìš© í† í°(ê¶Œì¥): X-Admin-Token
    x_admin = (request.headers.get("X-Admin-Token") or "").strip()

    # ë ˆê±°ì‹œ/ì¼ë°˜: Authorization: Bearer <token>
    supplied = credentials.credentials if credentials else ""
    if not supplied:
        # securityê°€ ëª» ë½‘ì•˜ì„ ë•Œ ëŒ€ë¹„
        auth = (request.headers.get("Authorization") or "").strip()
        if auth.lower().startswith("bearer "):
            supplied = auth[7:].strip()

    if (x_admin and x_admin in expected) or (supplied and supplied in expected):
        return True

    raise HTTPException(status_code=401, detail="invalid or missing token")

def _auth_header_or_qs_ok(request: Request) -> bool:
    expected = [(os.environ.get("ADMIN_TOKEN") or "").strip(),
                (os.environ.get("API_TOKEN") or "").strip()]
    expected = [x for x in expected if x]
    if not expected:
        return True
    supplied = None
    hdr = request.headers.get("authorization") or request.headers.get("Authorization")
    if hdr and hdr.lower().startswith("bearer "):
        supplied = hdr.split(" ", 1)[1].strip()
    qs = request.query_params.get("token")
    if (supplied and supplied in expected) or (qs and qs in expected):
        return True
    raise HTTPException(status_code=401, detail="invalid or missing token")

# ---------------------------------------------------------------------------
# Paths / Constants
# ---------------------------------------------------------------------------
BASE = Path(__file__).resolve().parent  # admin/

def _detect_root() -> Path:
    """
    Orchestrator root auto-detection:
    prefer admin/.. â†’ admin/ â†’ admin/../..
    """
    cands = [BASE.parent, BASE, BASE.parent.parent]
    for r in cands:
        if (r / "orchestrator.py").exists():
            return r
    return BASE.parent

ROOT = _detect_root()
ARCHIVE = ROOT / "archive"
TOOLS   = ROOT / "tools"
ORCH    = ROOT / "orchestrator.py"

SEL_COMM = ROOT / "selected_community.json"
SEL_WORK = ROOT / "data" / "selected_keyword_articles.json"
SEL_PUB  = ROOT / "selected_articles.json"

# Enriched summaries output dir
ENRICHED_DIR = ARCHIVE / "enriched"
CAND_COMM = [SEL_COMM, ROOT / "archive" / "selected_community.json"]

INDEX_HTML = BASE / "index.html"
INDEX_LITE = BASE / "index_lite_black.html"
CONFIG_FILE = ROOT / "config.json"

# Cloud Run support
ARCHIVE_CLOUD = Path(os.getenv("ARCHIVE_DIR", "/tmp/archive"))
IS_CLOUD = bool(os.getenv("K_SERVICE"))

REPORT_DIR = (ARCHIVE_CLOUD / "reports") if IS_CLOUD else (BASE / "archive" / "reports")
ENRICH_DIR = (ARCHIVE_CLOUD / "enriched") if IS_CLOUD else (ARCHIVE / "enriched")
REPORT_DIR.mkdir(parents=True, exist_ok=True)
ENRICH_DIR.mkdir(parents=True, exist_ok=True)

ARCHIVE_ADMIN = BASE / "archive"   # admin\archive (report .md ì €ì¥ ìœ„ì¹˜)

# KPI / Gate defaults
KPI = {"selected": 0, "approved": 0, "published": 0}
GATE = {"gate_required": int(os.getenv("GATE_REQUIRED", "15"))}

# Directory to persist task logs. Logs are saved as <job_id>.log
TASK_LOG_DIR = ROOT / "logs" / "tasks"

# ---------------------------------------------------------------------------
# Logging
# ---------------------------------------------------------------------------
try:
    from logging_setup import setup_logger  # type: ignore
    logger = setup_logger("server", str(ROOT / "logs" / "server.log"))
except Exception:  # pragma: no cover
    import logging
    (ROOT / "logs").mkdir(parents=True, exist_ok=True)
    logging.basicConfig(
        filename=str(ROOT / "logs" / "server.log"),
        level=logging.INFO,
        format="%(asctime)s [%(levelname)s] %(message)s",
        encoding="utf-8",
    )
    logger = logging.getLogger("server")
    logger.info("fallback logger initialized")

# ---------------------------------------------------------------------------
# FastAPI app
# ---------------------------------------------------------------------------
app.add_middleware(
    CORSMiddleware,
    allow_origins=[o.strip() for o in (os.getenv("ALLOWED_ORIGINS","").split(",")) if o.strip()] or ["https://admin.example.com"],
    allow_methods=["GET","POST","PATCH","OPTIONS"],
    allow_headers=["Authorization","Content-Type"],
    allow_credentials=True,
)

# UTF-8 ê°•ì œ í—¤ë”(ì •ì  .mdë„ í¬í•¨)
@app.middleware("http")
async def _force_utf8_markdown(request: Request, call_next):
    resp = await call_next(request)
    ctype = (resp.headers.get("content-type") or "").lower()
    if ctype.startswith("text/markdown") and "charset=" not in ctype:
        resp.headers["content-type"] = "text/markdown; charset=utf-8"
    return resp


# Static mounts
try:
    _paths = [getattr(r, "path", None) for r in getattr(app, "routes", [])]
    if "/archive/reports" not in _paths:
        app.mount("/archive/reports", StaticFiles(directory=str(REPORT_DIR)), name="archive-reports")
    if "/archive/enriched" not in _paths:
        app.mount("/archive/enriched", StaticFiles(directory=str(ENRICH_DIR)), name="archive-enriched")
    if "/archive" not in _paths:
        root_for_browse = (ARCHIVE_CLOUD if IS_CLOUD else BASE / "archive")
        root_for_browse.mkdir(parents=True, exist_ok=True)
        app.mount("/archive", StaticFiles(directory=str(root_for_browse)), name="archive-root")
except Exception:
    pass

# === ë³´í˜¸ ë‹¤ìš´ë¡œë“œ ì—”ë“œí¬ì¸íŠ¸ (/api/archive/...) ===
@app.get("/api/archive/{path:path}")
def download_archive(path: str, request: Request):
    """
    ë³´í˜¸ëœ ë‹¤ìš´ë¡œë“œ(í—¤ë” Bearer ë˜ëŠ” ?token= í—ˆìš©).
    ê³µê°œ ì •ì  ë¸Œë¼ìš°ì§•(/archive)ì€ ìœ ì§€í•˜ë˜, ì‹¤ì‚¬ìš© ë§í¬ëŠ” ì´ ê²½ë¡œ ê¶Œì¥.
    """
    _auth_header_or_qs_ok(request)  # í—¤ë” ìš°ì„ , ?token í—ˆìš©

    base = (ARCHIVE_CLOUD if IS_CLOUD else (BASE / "archive")).resolve()
    full = (base / path).resolve()

    # ê²½ë¡œ ì´íƒˆ ë°©ì§€(../../ ì°¨ë‹¨)
    if not str(full).startswith(str(base)) or not full.is_file():
        raise HTTPException(status_code=404, detail="File not found")

    return FileResponse(str(full), filename=full.name)

# ---------------------------------------------------------------------------
# Health & Misc
# ---------------------------------------------------------------------------
@app.get("/health", include_in_schema=False)
async def health():
    return {"status": True}

@app.get("/api/db/mode")
def db_mode():
    return {"mode": MODE}

# --- Approve UI opener -------------------------------------------------------
@app.post("/api/approve-ui/start")
def approve_ui_start(request: Request, authorized: bool = Depends(authorize)):
    """
    ìŠ¹ì¸ UIê°€ ì—´ë¦´ ë•Œ UI URLê³¼ í˜„ì¬ ìŠ¤ëƒ…ìƒ·(ë‚ ì§œ/í‚¤ì›Œë“œ/ê²Œì´íŠ¸)ì„ ì•ˆë‚´.
    - UIëŠ” ë°˜í™˜ëœ ui_urlì„ ìƒˆ ì°½ìœ¼ë¡œ open (index.htmlì˜ startApprove ì‚¬ìš©).
    """
    snap = _get_work_snapshot()  # {"date","keyword","articles":[...]}
    ui_url = (os.getenv("APPROVE_UI_URL") or str(request.base_url).rstrip("/"))
    return _ok(
        "approve_ui_start",
        ui_url=ui_url,
        date=snap.get("date"),
        keyword=snap.get("keyword", ""),
        gate_required=int(GATE.get("gate_required", 15)),
    )

@app.get("/api/db/ping")
def db_ping():
    """Graceful DB availability check. Falls back to not-connected if missing."""
    global _engine
    if MODE != "cloud" or make_engine is None:
        return {"ok": True, "mode": MODE, "db": "not-connected"}
    if _engine is None:
        _engine = make_engine()
    with _engine.connect() as c:
        c.exec_driver_sql("SELECT 1;")
    return {"ok": True, "mode": "cloud", "db": "postgresql"}

@app.get("/.well-known/appspecific/com.chrome.devtools.json")
def devtools_config():
    return Response(content="{}", media_type="application/json", status_code=200)

@app.get("/favicon.ico")
def favicon_blank():
    return Response(status_code=204)

@app.get("/")
def index():
    p = INDEX_HTML if INDEX_HTML.exists() else (INDEX_LITE if INDEX_LITE.exists() else None)
    if p:
        # ìºì‹œ ë¬´ì‹œ í—¤ë”ë¡œ êµ¬íŒ ìºì‹œ ì™„ì „ ì°¨ë‹¨
        return HTMLResponse(
            p.read_text(encoding="utf-8"),
            headers={"Cache-Control": "no-store, no-cache, must-revalidate"}
        )
    return HTMLResponse("<h1>QualiJournal Admin</h1><p>index.htmlì´ ì—†ìŠµë‹ˆë‹¤.</p>",
                        headers={"Cache-Control": "no-store"})

# ë””ë²„ê·¸: Cloud Run ëŸ°íƒ€ì„/ë¦¬ë¹„ì „/ì»¤ë°‹ í‘œì‹œ
@app.get("/api/debug/runtime")
def runtime_info(authorized: bool = Depends(authorize)):
    kst = _dt.datetime.utcnow().astimezone(_dt.timezone(_dt.timedelta(hours=9)))
    return JSONResponse({
        "service": os.getenv("K_SERVICE", ""),
        "revision": os.getenv("K_REVISION", ""),
        "commit": os.getenv("COMMIT_SHA", os.getenv("BUILD_ID","")),
        "time_kst": kst.strftime("%Y-%m-%d %H:%M")
    })


_LAST_BACKUP = {"ts":0,"ok":False,"size_md":0,"size_csv":0}

class BackupNotify(BaseModel):
    ok: bool
    ts: int = Field(ge=0)
    size_md: int = Field(default=0, ge=0)
    size_csv: int = Field(default=0, ge=0)


@app.post("/api/backup/notify")
def backup_notify(req: BackupNotify, authorized: bool = Depends(authorize)):
    global _LAST_BACKUP
    # Pydantic v2(model_dump) / v1(dict) ëª¨ë‘ í˜¸í™˜
    data = req.model_dump() if hasattr(req, "model_dump") else req.dict()
    # íƒ€ì… ì•ˆì „í™”
    data = {
        "ok": bool(data.get("ok")),
        "ts": int(data.get("ts") or 0),
        "size_md": int(data.get("size_md") or 0),
        "size_csv": int(data.get("size_csv") or 0),
    }
    _LAST_BACKUP = data
    return {"ok": True}


@app.get("/api/backup/status")
def backup_status():
    return _LAST_BACKUP

# ë””ë²„ê·¸: Cloud Runì—ì„œ ì‹¤ì œë¡œ ì–´ë–¤ HTML íŒŒì¼ì„ ì„œë¹™í•˜ëŠ”ì§€ í™•ì¸
@app.get("/api/debug/html_info")
def html_info(authorized: bool = Depends(authorize)):
    from hashlib import md5
    info = {}
    p = INDEX_HTML if INDEX_HTML.exists() else (INDEX_LITE if INDEX_LITE.exists() else None)
    if p and p.exists():
        data = p.read_bytes()
        info = {
            "path": str(p),
            "exists": True,
            "size": len(data),
            "md5": md5(data).hexdigest(),
            "snippet": data[:120].decode("utf-8", errors="ignore")
        }
    else:
        info = {"path": None, "exists": False}
    return JSONResponse({
        "base": str(BASE),
        "k_service": os.getenv("K_SERVICE", ""),
        "html": info
    })

# ---------------------------------------------------------------------------
# Helpers

# === UI ì—°ë™ìš© ì‘ë‹µ ìœ í‹¸(ë©±ë“±) ===
def _now_ms() -> int:
    return int(time.time() * 1000)

def _ok(op: str, **kw):
    return JSONResponse(
        {"ok": True, "op": op, "ts": int(time.time()), **kw},
        headers={"Cache-Control": "no-store"},
    )

def _err(op: str, msg: str, **kw):
    return JSONResponse(
        {"ok": False, "op": op, "error": str(msg), "ts": int(time.time()), **kw},
        headers={"Cache-Control": "no-store"},
    )

# ---------------------------------------------------------------------------
def _task_log_dir() -> Path:
    d = (ROOT / "logs" / "tasks")
    d.mkdir(parents=True, exist_ok=True)
    return d

def _read_json(p: Path) -> dict:
    if not p.exists():
        return {}
    try:
        return json.loads(p.read_text(encoding="utf-8-sig"))
    except Exception:
        try:
            return json.loads(p.read_bytes().decode("utf-8", errors="ignore").lstrip("\ufeff"))
        except Exception:
            return {}

def _write_json(p: Path, obj: dict):
    p.parent.mkdir(parents=True, exist_ok=True)
    tmp = p.with_suffix(".tmp")
    tmp.write_text(json.dumps(obj, ensure_ascii=False, indent=2), encoding="utf-8")
    tmp.replace(p)

def _slug_kw(s: str) -> str:
    return "".join(ch if ch.isalnum() or ch in "-_." else "-" for ch in (s or "")).strip("-").upper()

def _ensure_id(item: dict) -> str:
    """Ensure a stable unique id in article dict."""
    if not isinstance(item, dict):
        return ""
    if item.get("id"):
        return str(item["id"])
    base = (item.get("url") or item.get("link") or item.get("title") or "").strip()
    if not base:
        base = json.dumps(item, ensure_ascii=False, sort_keys=True)
    h = hashlib.md5(base.encode("utf-8", "ignore")).hexdigest()
    item["id"] = h
    return h

def _load_cfg() -> dict:
    try:
        return json.loads(CONFIG_FILE.read_text(encoding="utf-8"))
    except Exception:
        return {}

def _save_cfg(obj: dict):
    CONFIG_FILE.parent.mkdir(parents=True, exist_ok=True)
    CONFIG_FILE.write_text(json.dumps(obj, ensure_ascii=False, indent=2), encoding="utf-8")

def _generate_summary_md(date: str, keyword: str, articles: List[dict], *, selected: bool = False) -> Path:
    """Create a Markdown summary file under ENRICHED_DIR and return its path."""
    slug = _slug_kw(keyword or "").strip()
    suffix = "selected" if selected else "all"
    fname = f"{date}_{slug}_{suffix}.md" if slug else f"{date}_{suffix}.md"
    ENRICHED_DIR.mkdir(parents=True, exist_ok=True)
    out_path = ENRICHED_DIR / fname

    header_kw = slug.replace("_", " ") if slug else ""
    header_suffix = "ì„ ì •ë³¸" if selected else "ì „ì²´"
    title_parts = ["QualiNews", date]
    if header_kw:
        title_parts.append(header_kw)
    title_parts.append(f"({header_suffix})")
    lines: List[str] = [" â€” ".join(title_parts), ""]

    for i, art in enumerate(articles, 1):
        title = art.get("title") or art.get("headline") or "(no title)"
        url   = art.get("url") or art.get("link") or ""
        summary = art.get("summary") or art.get("ko_summary") or art.get("desc") or ""
        note  = art.get("editor_note") or ""
        lines.append(f"### {i}. {title}")
        if url:
            lines.append(f"- ì›ë¬¸: {url}")
        if summary:
            lines.append(f"- ìš”ì•½: {summary}")
        if note:
            lines.append(f"- í¸ì§‘ì ì½”ë©˜íŠ¸: {note}")
        lines.append("")
    md = "\n".join(lines)
    out_path.write_text(md, encoding="utf-8")
    return out_path

def _run_orch(*args: str) -> dict:
    """Run orchestrator.py with UTF-8 safety; return {'ok', 'stdout', 'stderr', 'cmd'}."""
    py  = PYEXE
    env = os.environ.copy()
    env.setdefault("PYTHONIOENCODING", "utf-8")

    # find script
    script = None
    if ORCH.exists():
        script = ORCH
    elif (TOOLS / "orchestrator.py").exists():
        script = TOOLS / "orchestrator.py"
    elif (ROOT / "orchestrator.py").exists():
        script = ROOT / "orchestrator.py"

    if not script:
        return {
            "ok": False,
            "stdout": "",
            "stderr": "orchestrator.py not found in image. (ë¹Œë“œ ì»¨í…ìŠ¤íŠ¸ë¥¼ repo ë£¨íŠ¸ë¡œ ì¡ì•˜ëŠ”ì§€ í™•ì¸)",
            "cmd": f"{py} orchestrator.py {' '.join(args)}"
        }

    cp = subprocess.run(
        [py, str(script), *args],
        cwd=str(ROOT),
        capture_output=True,
        text=True,
        encoding="utf-8",
        errors="replace",
        env=env,
    )
    try:
        logger.info("run orch: %s rc=%s", " ".join([str(script), *args]), cp.returncode)
        if cp.stderr:
            logger.warning("orch stderr: %s", cp.stderr.strip().replace("\n", " ")[:200])
    except Exception:
        pass
    return {"ok": cp.returncode == 0, "stdout": cp.stdout, "stderr": cp.stderr,
            "cmd": " ".join([str(script), *args])}

def _run_py(script_name: str, args: List[str] | None = None):
    """Run tools/*.py or project-root scripts with UTF-8 safety."""
    candidates = [TOOLS / script_name, ROOT / script_name]
    target = next((p for p in candidates if p.exists()), None)
    if not target:
        return 127, "", f"{script_name} not found"
    env = os.environ.copy()
    env.setdefault("PYTHONIOENCODING", "utf-8")
    cp = subprocess.run(
        [PYEXE, str(target), *(args or [])],
        cwd=str(ROOT),
        capture_output=True,
        text=True,
        encoding="utf-8",
        errors="replace",
        env=env
    )
    return cp.returncode, cp.stdout, cp.stderr

def _get_community_snapshot() -> dict:
    obj = {}
    arts: list[dict] = []
    for p in CAND_COMM:
        if p.exists():
            obj = _read_json(p)
            arts = obj.get("articles", [])
            if arts:
                break
    if not arts:
        obj = _read_json(SEL_WORK)
        arts = [a for a in obj.get("articles", []) if a.get("type") == "community"] or obj.get("articles", [])
    for a in arts:
        _ensure_id(a); a.setdefault("approved", False); a.setdefault("editor_note", "")
        a.setdefault("score", a.get("score", 0)); a.setdefault("source", a.get("source", ""))
    date = obj.get("date") or _dt.date.today().isoformat()
    keyword = obj.get("keyword", "")
    return {"date": date, "keyword": keyword, "articles": arts}

def _get_work_snapshot() -> dict:
    """Load selected_keyword_articles.json as work snapshot."""
    obj = _read_json(SEL_WORK) or {}
    arts = obj.get("articles", []) or []
    for a in arts:
        _ensure_id(a)
        a.setdefault("approved", False)
        a.setdefault("editor_note", "")
        a.setdefault("state", (a.get("state") or "").lower() or "candidate")
    date = obj.get("date") or _dt.date.today().isoformat()
    keyword = obj.get("keyword", "")
    return {"date": date, "keyword": keyword, "articles": arts}

def _sync_after_save() -> dict:
    """Sync selected -> publish (tools/sync_selected_for_publish.py if exists; else safe merge)."""
    py  = PYEXE
    script = TOOLS / "sync_selected_for_publish.py"
    if script.exists():
        try:
            env = os.environ.copy(); env.setdefault("PYTHONIOENCODING", "utf-8")
            cp = subprocess.run([py, str(script)], cwd=str(ROOT), capture_output=True, text=True, encoding="utf-8", errors="replace", timeout=120, env=env)
            return {"ok": cp.returncode == 0, "stdout": cp.stdout, "stderr": cp.stderr}
        except Exception as e:
            return {"ok": False, "stderr": str(e)}
    # fallback merge (work -> publish only approved)
    work = _read_json(SEL_WORK) or {}
    pub  = _read_json(SEL_PUB)  or {}
    merged: Dict[str, dict] = {}
    for art in pub.get("articles", []) or []:
        try: _ensure_id(art); merged[art["id"]] = art
        except Exception: continue
    for art in work.get("articles", []) or []:
        if not art.get("approved"): continue
        try: _ensure_id(art); aid = art["id"]
        except Exception: continue
        if aid in merged:
            ex = merged[aid]
            ex["approved"] = art.get("approved", ex.get("approved"))
            ex["editor_note"] = art.get("editor_note", ex.get("editor_note", ""))
            if art.get("pinned") is not None: ex["pinned"] = art.get("pinned")
            if art.get("pin_ts"): ex["pin_ts"] = art.get("pin_ts")
            if art.get("selected") is not None: ex["selected"] = art.get("selected")
        else:
            merged[aid] = art
    date_val = work.get("date") or pub.get("date") or _dt.date.today().isoformat()
    out = {"date": date_val, "articles": list(merged.values())}
    _write_json(SEL_PUB, out)
    return {"ok": True, "stdout": "fallback merge ok"}

def _rollover_archive_if_needed(keyword: str) -> Optional[List[str]]:
    date = _dt.date.today().isoformat()
    base = f"{date}_{_slug_kw(keyword)}"
    created = []; ARCHIVE.mkdir(parents=True, exist_ok=True)
    for ext in (".html", ".md", ".json"):
        p = ARCHIVE / f"{base}{ext}"
        if p.exists():
            ts = _dt.datetime.now().strftime("%H%M")
            newp = ARCHIVE / f"{base}_{ts}{ext}"
            p.rename(newp); created.append(str(newp))
    return created or None

def _latest_published_paths(keyword: str) -> List[str]:
    date = _dt.date.today().isoformat()
    base = f"{date}_{_slug_kw(keyword)}"
    out = []
    for ext in (".html", ".md", ".json"):
        p = ARCHIVE / f"{base}{ext}"
        if p.exists(): out.append(str(p))
    return out

def _read_any_items(root: Path):
    """
    Priority:
      1) data/selected_keyword_articles.json  {date, keyword, items:[...]}
      2) selected_articles.json               [ ... ]
      3) data_selected_articles.json          [ ... ]
    """
    cand = [
        root / "data" / "selected_keyword_articles.json",
        root / "selected_articles.json",
        root / "data_selected_articles.json",
    ]
    for p in cand:
        if p.exists():
            try:
                obj = json.loads(p.read_text(encoding="utf-8"))
                if isinstance(obj, dict) and "items" in obj:
                    return obj.get("items") or [], obj.get("date"), obj.get("keyword")
                if isinstance(obj, list):
                    return obj, None, None
            except Exception:
                pass
    return [], None, None

# ---------------------------------------------------------------------------
# In-memory Async Task Manager (+SSE)
# ---------------------------------------------------------------------------
class Task:
    def __init__(self, kind: str, args: list[str]):
        self.id = secrets.token_hex(8)
        self.kind = kind
        self.args = args
        self.status = "pending"          # pending|running|done|error|canceled
        self.created_at = time.time()
        self.started_at: float | None = None
        self.ended_at: float | None = None
        self.exit_code: int | None = None
        self.logs: list[str] = []
        self._cancel = False
        self._lock = threading.Lock()
        # persistent log file path
        global TASK_LOG_DIR
        try:
            TASK_LOG_DIR.mkdir(parents=True, exist_ok=True)
            self.log_file: Path | None = TASK_LOG_DIR / f"{self.id}.log"
            self.log_file.write_text("", encoding="utf-8")
        except Exception:
            self.log_file = None

    def append(self, line: str):
        with self._lock:
            ts = _dt.datetime.now().strftime("%H:%M:%S")
            msg = f"[{ts}] {line}"
            self.logs.append(msg)
            # persist to disk
            try:
                if self.log_file:
                    with self.log_file.open("a", encoding="utf-8") as fp:
                        fp.write(msg + "\n")
            except Exception:
                pass

class TaskManager:
    def __init__(self, keep=50):
        self.keep = keep
        self.jobs: dict[str, Task] = {}
        self._lock = threading.Lock()

    def add(self, t: Task):
        with self._lock:
            self.jobs[t.id] = t
            if len(self.jobs) > self.keep:
                for jid in sorted(self.jobs.keys())[:-self.keep]:
                    self.jobs.pop(jid, None)

    def get(self, jid: str) -> Task | None:
        return self.jobs.get(jid)

TM = TaskManager()

def _run_task(task: Task):
    task.status = "running"; task.started_at = time.time()

    def run_cmd(cmd: list[str]) -> int:
        task.append(f"$ {' '.join(cmd)}")
        p = subprocess.Popen(
            cmd, cwd=str(ROOT),
            stdout=subprocess.PIPE, stderr=subprocess.STDOUT,
            text=True, encoding="utf-8"
        )
        while True:
            if task._cancel:
                p.kill(); task.append("! canceled"); return 1
            line = p.stdout.readline()
            if not line and p.poll() is not None: break
            if line: task.append(line.rstrip())
        return p.returncode if p.returncode is not None else 0

    try:
        py = PYEXE
        if task.kind == "daily":
            rc1 = run_cmd([py, str(ORCH), "--collect-community"])
            rc2 = run_cmd([py, str(ORCH), "--publish-community", "--format", "all"])
            rc3 = run_cmd([py, str(ORCH), "--publish", "--format", "all"])
            rc = max(rc1, rc2, rc3)
        elif task.kind == "community":
            rc1 = run_cmd([py, str(ORCH), "--collect-community"])
            rc2 = run_cmd([py, str(ORCH), "--publish-community", "--format", "all"])
            rc = max(rc1, rc2)
        elif task.kind == "keyword":
            kw = task.args[0] if task.args else ""
            if not kw:
                raise RuntimeError("keyword required")
            rc1 = run_cmd([py, str(ORCH), "--collect-keyword", kw])
            rc2 = run_cmd([py, str(ORCH), "--approve-keyword", kw, "--approve-keyword-top", "15"])
            rc3 = run_cmd([py, str(ORCH), "--publish-keyword", kw])
            rc = max(rc1, rc2, rc3)
        else:
            raise RuntimeError(f"unknown kind: {task.kind}")
        task.exit_code = rc
        task.status = "done" if rc == 0 else "error"
        if rc != 0:
            task.append(f"! exit={rc}")
    except Exception as e:
        task.status = "error"
        task.append(f"! error: {e}")
    finally:
        task.ended_at = time.time()

# ---------------------------------------------------------------------------
# Tasks API (protected)
# ---------------------------------------------------------------------------
@app.post("/api/tasks/flow")
def create_flow(req: FlowReq, authorized: bool = Depends(authorize)):
    kind = (req.kind or "").lower().strip()
    args = [req.keyword] if kind == "keyword" else []
    t = Task(kind, args)
    TM.add(t)
    th = threading.Thread(target=_run_task, args=(t,), daemon=True)
    th.start()
    return {"job_id": t.id, "status": t.status, "kind": t.kind, "args": t.args}

# Register recent BEFORE parameterized route to avoid conflicts
@app.get("/api/tasks/recent", response_model=TasksRecent)
def tasks_recent(limit: int = Query(10, ge=1, le=50), authorized: bool = Depends(authorize)) -> TasksRecent:
    try:
        d = _task_log_dir()
        files = sorted(d.glob("*.log"), key=lambda p: p.stat().st_mtime, reverse=True)
        items = [TaskItem(id=p.stem, size=p.stat().st_size) for p in files[:limit]]
        return TasksRecent(items=items)
    except Exception:
        return TasksRecent(items=[])

@app.get("/api/tasks/{job_id}")
def get_task(job_id: str, authorized: bool = Depends(authorize)):
    t = TM.get(job_id)
    if not t:
        raise HTTPException(404, "job not found")
    return {
        "id": t.id,
        "kind": t.kind,
        "status": t.status,
        "created_at": t.created_at,
        "started_at": t.started_at,
        "ended_at": t.ended_at,
        "exit_code": t.exit_code,
        "lines": len(t.logs),
        "log_file": str(getattr(t, "log_file", "")) if getattr(t, "log_file", None) else None,
    }

@app.post("/api/tasks/{job_id}/cancel")
def cancel_task(job_id: str, authorized: bool = Depends(authorize)):
    t = TM.get(job_id)
    if not t:
        raise HTTPException(404, "job not found")
    t._cancel = True
    return {"ok": True}

@app.get("/api/tasks/{job_id}/stream")
async def stream_task(job_id: str, request: Request):
    _auth_header_or_qs_ok(request)  # í—¤ë” ë˜ëŠ” ?token= í—ˆìš©

    t = TM.get(job_id)
    if not t:
        raise HTTPException(404, "job not found")

    async def _gen():
        idx = 0
        while True:
            if idx < len(t.logs):
                chunk = "\n".join(t.logs[idx:])
                idx = len(t.logs)
                yield f"data: {chunk}\n\n"
            if t.status in ("done", "error", "canceled"):
                yield f"event: end\ndata: {t.status}\n\n"
                break
            await asyncio.sleep(0.5)

    return StreamingResponse(_gen(), media_type="text/event-stream")


# ---------------------------------------------------------------------------
# Gate config (GET/PATCH) â€” protected
# ---------------------------------------------------------------------------
@app.get("/api/config/gate_required")
async def get_gate_required(authorized: bool = Depends(authorize)):
    return {"gate_required": int(GATE.get("gate_required", 15))}

@app.patch("/api/config/gate_required")
async def set_gate_required(p: GatePatch, authorized: bool = Depends(authorize)):
    v = max(1, min(100, int(p.gate_required)))  # 1~100 clamp
    GATE["gate_required"] = v
    return {"ok": True, "gate_required": v}

# ---------------------------------------------------------------------------
# Report / Enrich / Export â€” protected
# ---------------------------------------------------------------------------
@app.get("/api/report")
def get_report(date: str | None = None, authorized: bool = Depends(authorize)):
    day = date or _dt.date.today().isoformat()
    ARCHIVE.mkdir(parents=True, exist_ok=True)
    items = []
    for p in ARCHIVE.glob(f"{day}*.*"):
        items.append({"name": p.name, "size": p.stat().st_size})
    for p in ARCHIVE.glob(f"community_{day}.*"):
        items.append({"name": p.name, "size": p.stat().st_size})
    for p in ARCHIVE.glob(f"daily_{day}.*"):
        items.append({"name": p.name, "size": p.stat().st_size})
    return {"date": day, "files": items}

@app.post("/api/report")
def post_report(payload: dict | None = Body(default=None), authorized: bool = Depends(authorize)):
    """
    UI ìŠ¤í”¼ë„ˆ/í† ìŠ¤íŠ¸ ì—°ë™ì„ ìœ„í•´ í•­ìƒ {"ok", "op", "path", "count", "duration_ms"} í˜•íƒœë¡œ ì‘ë‹µ.
    ì‹¤íŒ¨ ì‹œì—ë„ 200 JSONìœ¼ë¡œ {"ok":False,"error":...} ë°˜í™˜(í† ìŠ¤íŠ¸ìš©).
    """
    t0 = _now_ms()
    try:
        reports_dir = REPORT_DIR
        reports_dir.mkdir(parents=True, exist_ok=True)

        date = (payload or {}).get("date") or _dt.date.today().isoformat()

        candidates = [
            BASE / "data" / "selected_keyword_articles.json",  # items array
            ROOT / "selected_articles.json",                   # list
            BASE / "data_selected_articles.json",              # list
        ]
        items = []
        keyword = "report"

        def _slug(s: str) -> str:
            return re.sub(r"[^A-Za-z0-9_-]+", "_", (s or "")).strip("_") or "report"

        for p in candidates:
            if p.exists():
                try:
                    obj = json.loads(p.read_text(encoding="utf-8"))
                    if isinstance(obj, dict) and "items" in obj:
                        items = obj.get("items") or []
                        keyword = _slug(obj.get("keyword") or keyword)
                    elif isinstance(obj, list):
                        items = obj
                    break
                except Exception:
                    pass

        def _esc(s): return re.sub(r"[\r\n]+", " ", str(s or "")).strip()
        lines = [f"# {date} Â· {keyword.upper()} Â· Daily Report", ""]
        if items:
            for i, it in enumerate(items, 1):
                t = _esc(it.get("title") or it.get("headline") or "(ì œëª© ì—†ìŒ)")
                u = _esc(it.get("url") or it.get("link") or "")
                se = _esc(it.get("summary_en") or it.get("summary") or "")
                sk = _esc(it.get("summary_ko") or it.get("summary_kr") or "")
                note = _esc(it.get("editor_note") or "")
                lines.append(f"## {i}. {t}")
                if u:   lines.append(f"- ì›ë¬¸: {u}")
                if se:  lines.append(f"- ìš”ì•½(EN): {se}")
                if sk:  lines.append(f"- ìš”ì•½(KO): {sk}")
                if note:lines.append(f"- ì½”ë©˜íŠ¸: {note}")
                lines.append("")
        else:
            lines += ["(ìˆ˜ì§‘ëœ ê¸°ì‚¬ ì—†ìŒ)", ""]

        out = reports_dir / f"{date}_{keyword}_report.md"
        out.write_text("\n".join(lines), encoding="utf-8")
        rel = f"archive/reports/{out.name}"
        return _ok("report", path=rel, count=len(items), duration_ms=_now_ms()-t0)
    except Exception as e:
        return _err("report", str(e), duration_ms=_now_ms()-t0)


@app.post("/api/enrich/keyword")
def enrich_keyword(req: EnrichReq | None = Body(default=None), authorized: bool = Depends(authorize)):
    t0 = _now_ms()
    req = req or EnrichReq()   # â† ë³¸ë¬¸ ì—†ì„ ë•Œ ê¸°ë³¸ê°’
    try:
        items, date, kw_in = _read_any_items(BASE)
        date = req.date or date or _dt.date.today().isoformat()
        kw   = (req.keyword or kw_in or "report")
        arts = items or []
        out_path = _generate_summary_md(date, kw, arts, selected=False)
        web_path = f"archive/enriched/{out_path.name}"
        return _ok("enrich_keyword", path=web_path, count=len(arts), duration_ms=_now_ms()-t0)
    except Exception as e:
        return _err("enrich_keyword", str(e), duration_ms=_now_ms()-t0)


@app.post("/api/enrich/selection")
def enrich_selection(req: EnrichReq | None = Body(default=None), authorized: bool = Depends(authorize)):
    t0 = _now_ms()
    req = req or EnrichReq()
    try:
        items, date, kw_in = _read_any_items(BASE)
        date = req.date or date or _dt.date.today().isoformat()
        kw   = (req.keyword or kw_in or "report")

        def _is_selected(a: dict) -> bool:
            s = (a.get("state") or "").lower()
            return bool(a.get("selected") or a.get("approved") or s in ("published", "ready"))

        arts = [a for a in (items or []) if _is_selected(a)]
        out_path = _generate_summary_md(date, kw, arts, selected=True)
        web_path = f"archive/enriched/{out_path.name}"
        return _ok("enrich_selection", path=web_path, count=len(arts), duration_ms=_now_ms()-t0)
    except Exception as e:
        return _err("enrich_selection", str(e), duration_ms=_now_ms()-t0)


@app.get("/api/export/{fmt}")
def export_fmt(fmt: str, date: str | None = None, preview: bool = Query(False), authorized: bool = Depends(authorize)):
    """
    Export final selection or community articles.
    If fmt is 'md' or 'csv', export selected_articles.json as Markdown or CSV.
    Otherwise, fallback to community export based on archive/community_date.json.
    """
    day = date or _dt.date.today().isoformat()
    fmt_lower = fmt.lower()

    # final selection export
    if fmt_lower in ("md", "csv"):
        data = _read_json(SEL_PUB)
        articles = data.get("articles", []) or []
        kw = data.get("keyword", "") or ""
        if fmt_lower == "md":
            lines = [f"# QualiNews â€” {day} â€” {kw}", ""]
            for i, a in enumerate(articles, 1):
                title = a.get("title", "(no title)")
                url   = a.get("url") or a.get("link") or ""
                summ  = a.get("summary") or a.get("ko_summary") or a.get("desc") or ""
                note  = a.get("editor_note") or ""
                lines.append(f"### {i}. {title}")
                if url:  lines.append(f"- ì›ë¬¸: {url}")
                if summ: lines.append(f"- ìš”ì•½: {summ}")
                if note: lines.append(f"- í¸ì§‘ì ì½”ë©˜íŠ¸: {note}")
                lines.append("")
            md = "\n".join(lines)
            fname = f"quali_{day}_{_slug_kw(kw)}.md"
            headers = {} if preview else {"Content-Disposition": f'attachment; filename="{fname}"'}
            return Response(content=md, media_type="text/markdown; charset=utf-8", headers=headers)
        elif fmt_lower == "csv":
            buf = io.StringIO()
            w = csv.writer(buf)
            w.writerow(["title","url","source","date","score","approved","editor_note","summary"])
            for a in articles:
                w.writerow([
                    a.get("title",""), a.get("url") or a.get("link") or "",
                    a.get("source",""), a.get("date",""),
                    a.get("score",""), a.get("approved",""),
                    a.get("editor_note",""), a.get("summary") or a.get("ko_summary") or a.get("desc","")
                ])
            csv_text = buf.getvalue()
            data_out  = "\ufeff" + csv_text  # add BOM for Excel
            fname = f"quali_{day}_{_slug_kw(kw)}.csv"
            headers = {"Content-Disposition": f'attachment; filename="{fname}"'}
            return Response(content=data_out, media_type="text/csv; charset=utf-8", headers=headers)

    # fallback community export
    cj = ARCHIVE / f"community_{day}.json"
    if not cj.exists():
        raise HTTPException(404, "community json not found")
    obj = _read_json(cj); arts = obj.get("articles", [])
    if fmt_lower == "md":
        lines = [f"# Community â€” {day}", ""]
        for a in arts:
            title = a.get("title") or "(no title)"
            url = a.get("url") or "#"
            meta = f"ğŸ‘{a.get('upvotes',0)} Â· ğŸ’¬{a.get('comments',0)} Â· ğŸ‘€{a.get('views','-')}"
            lines.append(f"- [{title}]({url})  \n  {meta} Â· {a.get('source','')}")
        md = "\n".join(lines) + "\n"
        return Response(content=md, media_type="text/markdown; charset=utf-8")
    if fmt_lower == "csv":
        buf = io.StringIO()
        w = csv.writer(buf)
        w.writerow(["title","url","source","upvotes","comments","views"])
        for a in arts:
            w.writerow([a.get("title",""), a.get("url",""), a.get("source",""),
                        a.get("upvotes",0), a.get("comments",0), a.get("views","")])
        data = "\ufeff" + buf.getvalue()  # UTF-8 BOM
        return Response(content=data, media_type="text/csv; charset=utf-8",
                        headers={"Content-Disposition": f'attachment; filename="community_{day}.csv"'})
    raise HTTPException(400, "unsupported format")

# --- explicit aliases for stability ---
@app.get("/api/export/md")
def export_md(preview: bool = Query(False), authorized: bool = Depends(authorize)):
    return export_fmt(fmt="md", preview=preview, authorized=authorized)

# (ì„ íƒ) ë ˆê±°ì‹œ/ì§§ì€ ê²½ë¡œë„ í—ˆìš©
@app.get("/export/md")
def export_md_alias(preview: bool = Query(False), authorized: bool = Depends(authorize)):
    return export_fmt(fmt="md", preview=preview, authorized=authorized)

@app.get("/export/csv")
def export_csv_alias(authorized: bool = Depends(authorize)):
    return export_fmt(fmt="csv", authorized=authorized)


# ---------------------------------------------------------------------------
# Community / Items / Publish â€” protected
# ---------------------------------------------------------------------------
class SaveItem(BaseModel):
    id: str
    approved: bool
    editor_note: str = ""

class SavePayload(BaseModel):
    changes: List[SaveItem]

@app.get("/api/community")
def api_get_community(only_pending: bool = Query(True), authorized: bool = Depends(authorize)):
    snap = _get_community_snapshot()
    arts = snap["articles"]
    if only_pending:
        arts = [a for a in arts if not a.get("approved")]
    total = len(snap["articles"])
    approved = sum(1 for a in snap["articles"] if a.get("approved"))
    return {"date": snap["date"], "keyword": snap.get("keyword",""), "total": total,
            "approved": approved, "pending": total-approved, "articles": arts}

@app.post("/api/community/save")
def api_save(payload: SavePayload, authorized: bool = Depends(authorize)):
    target = SEL_COMM if SEL_COMM.exists() else SEL_WORK
    obj = _read_json(target); items = obj.get("articles", []); idx = {_ensure_id(a): a for a in items}
    changed = 0
    for c in payload.changes:
        row = idx.get(c.id)
        if not row:
            continue
        if row.get("approved") != c.approved or (row.get("editor_note","") != c.editor_note):
            row["approved"] = c.approved
            row["editor_note"] = c.editor_note
            changed += 1
    if changed:
        _write_json(target, obj)
    sync = _sync_after_save()
    return {"saved": changed, "synced": sync.get("ok", False), "sync_log": sync}

@app.get("/api/items")
def api_items(state: str = Query("ready"), date: str | None = None, keyword: str | None = None,
              authorized: bool = Depends(authorize)):
    """
    state=candidate|ready|rejected|published
    date/keyword are hints (current file structure mainly uses state filter).
    """
    snap = _get_work_snapshot()
    s = (state or "").lower().strip()
    arts = snap["articles"]
    if s:
        if s == "published":
            pub = _read_json(SEL_PUB) or {}
            items = pub.get("articles", []) or []
            return {"date": pub.get("date", snap["date"]), "keyword": snap.get("keyword",""), "state": s, "items": items}
        items = [a for a in arts if (a.get("state","").lower() == s)]
    else:
        items = arts
    return {"date": snap["date"], "keyword": snap.get("keyword",""), "state": s or "all", "items": items}

@app.post("/api/items/{item_id}/publish")
def api_items_publish(item_id: str, req: PublishOneReq, authorized: bool = Depends(authorize)):
    """
    Publish single item: update approval and notes in work file then sync to publish file.
    """
    work = _read_json(SEL_WORK) or {}
    arts = work.get("articles", []) or []
    idx = None
    for i, a in enumerate(arts):
        try:
            if _ensure_id(a) == item_id:
                idx = i
                break
        except Exception:
            continue
    if idx is None:
        raise HTTPException(status_code=404, detail="item not found")

    a = arts[idx]
    a["approved"] = bool(req.approve)
    a["selected"] = True
    a["editor_note"] = req.editor_note or a.get("editor_note","")
    a["state"] = "published"

    work["articles"] = arts
    _write_json(SEL_WORK, work)

    sync = _sync_after_save()
    return {"ok": True, "synced": sync.get("ok", False), "item_id": item_id}

@app.post("/api/publish-keyword")
def api_publish(req: PublishReq, authorized: bool = Depends(authorize)):
    ARCHIVE.mkdir(parents=True, exist_ok=True)
    rollover = _rollover_archive_if_needed(req.keyword)
    out = _run_orch("--publish-keyword", req.keyword)
    outputs = _latest_published_paths(req.keyword)
    return {**out, "rolled_over": rollover or [], "created": outputs}

# Legacy sync flows (kept for compatibility)
@app.post("/api/flow/community")
def api_flow_comm(authorized: bool = Depends(authorize)):
    return _run_orch("--collect-community")

@app.post("/api/flow/daily")
def api_flow_daily(authorized: bool = Depends(authorize)):
    steps = [
        _run_orch("--collect-community"),
        _run_orch("--publish-community", "--format", "all"),
        _run_orch("--publish", "--format", "all"),
    ]
    ok = all(s.get("ok", True) for s in steps)
    return {"ok": ok, "steps": steps}

@app.post("/api/flow/keyword")
def api_flow_keyword(req: FlowKwReq, authorized: bool = Depends(authorize)):
    steps = []
    if req.use_external_rss:
        steps.append(_run_orch("--collect-keyword", req.keyword, "--use-external-rss"))
    else:
        steps.append(_run_orch("--collect-keyword", req.keyword))
    steps.append(_run_orch("--approve-keyword-top", "20", "--approve-keyword", req.keyword))
    steps.append(_sync_after_save())
    steps.append(_run_orch("--publish-keyword", req.keyword))
    ok = all(s.get("ok", True) for s in steps)
    return {"ok": ok, "steps": steps}

# ---------------------------------------------------------------------------
# Logs (protected + optional JWT)
# ---------------------------------------------------------------------------
@app.get("/api/logs")
def list_logs(authorized: bool = Depends(authorize), user: dict = Depends(verify_jwt_token)):
    logs_dir = ROOT / "logs"
    items = []
    if logs_dir.exists() and logs_dir.is_dir():
        for p in logs_dir.iterdir():
            if p.is_file() and p.suffix == ".log":
                try:
                    stat = p.stat()
                    items.append({"name": p.name, "size": stat.st_size,
                                  "modified": _dt.datetime.utcfromtimestamp(stat.st_mtime).isoformat()+"Z"})
                except Exception:
                    continue
    return {"items": items}

@app.get("/api/logs/{log_name}")
def get_log(log_name: str, lines: int = 200, authorized: bool = Depends(authorize), user: dict = Depends(verify_jwt_token)):
    path = ROOT / "logs" / log_name
    if not path.exists() or not path.is_file():
        raise HTTPException(404, "log not found")
    try:
        with open(path, "r", encoding="utf-8", errors="replace") as f:
            lines_data = f.readlines()
        content = "".join(lines_data[-int(lines):]) if (lines and lines > 0) else "".join(lines_data)
    except Exception as e:
        raise HTTPException(500, str(e))
    return PlainTextResponse(content, media_type="text/plain")

@app.get("/api/logs/{log_name}/download")
def download_log(log_name: str, authorized: bool = Depends(authorize), user: dict = Depends(verify_jwt_token)):
    path = ROOT / "logs" / log_name
    if not path.exists() or not path.is_file():
        raise HTTPException(404, "log not found")
    return FileResponse(str(path), filename=log_name, media_type="text/plain")

# ---------------------------------------------------------------------------
# KPI status â€” protected (fixes 'always authenticated' UI issue)
# ---------------------------------------------------------------------------
@app.get("/api/status")
async def get_status(
    date: str | None = None,
    keyword: str | None = None,
    authorized: bool = Depends(authorize),
):
    """
    ê´€ë¦¬ì KPI/ìƒíƒœ ì¡°íšŒ (í† í° ë³´í˜¸)
    - selection/community ì§‘ê³„, ê²Œì´íŠ¸ í†µê³¼ ì—¬ë¶€, íƒ€ì„ìŠ¤íƒ¬í”„ í¬í•¨
    - date/keyword ì¿¼ë¦¬ íŒŒë¼ë¯¸í„°ëŠ” ì„ íƒ(ì§€ì • ì—†ìœ¼ë©´ ìŠ¤ëƒ…ìƒ· ê°’ ì‚¬ìš©)
    """
    try:
        work = _get_work_snapshot()            # {"date","keyword","articles":[...]}
        arts = work.get("articles", []) or []

        # ìƒíƒœë³„ ì§‘ê³„
        state_counts = {"candidate": 0, "ready": 0, "rejected": 0}
        selection_total = len(arts)
        selection_approved = 0
        for a in arts:
            st = (a.get("state") or "candidate").lower()
            if st in state_counts:
                state_counts[st] += 1
            if a.get("approved") or a.get("selected") or st in ("ready", "published"):
                selection_approved += 1

        # ì»¤ë®¤ë‹ˆí‹° ì§‘ê³„
        comm = _get_community_snapshot()       # {"date","keyword","articles":[...]}
        community_total = len(comm.get("articles", []) or [])

        # ê²Œì´íŠ¸
        gate_required = int(GATE.get("gate_required", 15))
        gate_pass = bool(selection_approved >= gate_required)

        return {
            # ëˆ„ì  KPI(ì—†ìœ¼ë©´ 0)
            "selected": KPI.get("selected", 0) if isinstance(KPI, dict) else 0,
            "approved": KPI.get("approved", 0) if isinstance(KPI, dict) else 0,
            "published": KPI.get("published", 0) if isinstance(KPI, dict) else 0,

            # í˜„ì¬ ìŠ¤ëƒ…ìƒ· ì§‘ê³„
            "gate_required": gate_required,
            "ts": int(time.time()),

            "selection_total": selection_total,
            "selection_approved": selection_approved,
            "state_counts": state_counts,

            "community_total": community_total,
            "keyword_total": selection_total,   # ì„ íƒë³¸ ì „ì²´ë¥¼ í‚¤ì›Œë“œ ì´ëŸ‰ìœ¼ë¡œ ê°„ì£¼

            "gate_pass": gate_pass,

            # í‘œì‹œìš© ë©”íƒ€(ìš”ì²­ê°’ ìš°ì„  â†’ ìŠ¤ëƒ…ìƒ· ê°’ í´ë°±)
            "date": date or work.get("date"),
            "keyword": keyword or work.get("keyword", ""),
        }
    except Exception:
        # ì•ˆì „ í´ë°±(ì ˆëŒ€ 500 ì•ˆ ë‚´ê³  ìµœì†Œ ì •ë³´ ì œê³µ)
        return {
            "selected": KPI.get("selected", 0) if isinstance(KPI, dict) else 0,
            "approved": KPI.get("approved", 0) if isinstance(KPI, dict) else 0,
            "published": KPI.get("published", 0) if isinstance(KPI, dict) else 0,
            "gate_required": int(GATE.get("gate_required", 15)) if isinstance(GATE, dict) else 15,
            "ts": int(time.time()),
            "selection_total": 0,
            "selection_approved": 0,
            "state_counts": {"candidate": 0, "ready": 0, "rejected": 0},
            "community_total": 0,
            "keyword_total": 0,
            "gate_pass": False,
            "date": date,
            "keyword": keyword or "",
        }


# ---------------------------------------------------------------------------
if __name__ == "__main__":
    import uvicorn
    _port = int(os.getenv("PORT", "8080"))
    uvicorn.run(app, host="0.0.0.0", port=_port, log_level="info")
